{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba5cf71",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc92759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a737428",
   "metadata": {},
   "source": [
    "### Dataset Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a6ef7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text data\n",
    "data = pd.read_csv('E:\\Oasis_InfoByte\\CSV_FILES\\creditcard.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4caf399c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b5cedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>-1.593105</td>\n",
       "      <td>2.711941</td>\n",
       "      <td>-0.689256</td>\n",
       "      <td>4.626942</td>\n",
       "      <td>-0.924459</td>\n",
       "      <td>1.107641</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>0.510632</td>\n",
       "      <td>-0.682920</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>-0.150189</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>1.214756</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>1.164931</td>\n",
       "      <td>-0.711757</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>-1.221179</td>\n",
       "      <td>-1.545556</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>0.411614</td>\n",
       "      <td>0.063119</td>\n",
       "      <td>-0.183699</td>\n",
       "      <td>-0.510602</td>\n",
       "      <td>1.329284</td>\n",
       "      <td>0.140716</td>\n",
       "      <td>0.313502</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>-0.577252</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>-1.933849</td>\n",
       "      <td>-0.962886</td>\n",
       "      <td>-1.042082</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>1.962563</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>1.113981</td>\n",
       "      <td>2.897849</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>-1.040458</td>\n",
       "      <td>-0.031513</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>0.167430</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9       V10       V11       V12  \\\n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  4.356170 -1.593105  2.711941   \n",
       "284803  1.058415  0.024330  0.294869  0.584800 -0.975926 -0.150189  0.915802   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454 -0.484782  0.411614  0.063119   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087 -0.399126 -1.933849 -0.962886   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180 -0.915427 -1.040458 -0.031513   \n",
       "\n",
       "             V13       V14       V15       V16       V17       V18       V19  \\\n",
       "284802 -0.689256  4.626942 -0.924459  1.107641  1.991691  0.510632 -0.682920   \n",
       "284803  1.214756 -0.675143  1.164931 -0.711757 -0.025693 -1.221179 -1.545556   \n",
       "284804 -0.183699 -0.510602  1.329284  0.140716  0.313502  0.395652 -0.577252   \n",
       "284805 -1.042082  0.449624  1.962563 -0.608577  0.509928  1.113981  2.897849   \n",
       "284806 -0.188093 -0.084316  0.041333 -0.302620 -0.660377  0.167430 -0.256117   \n",
       "\n",
       "             V20       V21       V22       V23       V24       V25       V26  \\\n",
       "284802  1.475829  0.213454  0.111864  1.014480 -0.509348  1.436807  0.250034   \n",
       "284803  0.059616  0.214205  0.924384  0.012463 -1.016226 -0.606624 -0.395255   \n",
       "284804  0.001396  0.232045  0.578229 -0.037501  0.640134  0.265745 -0.087371   \n",
       "284805  0.127434  0.265245  0.800049 -0.163298  0.123205 -0.569159  0.546668   \n",
       "284806  0.382948  0.261057  0.643078  0.376777  0.008797 -0.473649 -0.818267   \n",
       "\n",
       "             V27       V28  Amount  Class  \n",
       "284802  0.943651  0.823731    0.77      0  \n",
       "284803  0.068472 -0.053527   24.79      0  \n",
       "284804  0.004455 -0.026561   67.88      0  \n",
       "284805  0.108821  0.104533   10.00      0  \n",
       "284806 -0.002415  0.013649  217.00      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c941c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb317fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3587c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data['Amount'] = scaler.fit_transform(data[['Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5349d38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "     Amount  Class  \n",
       "0  0.244964      0  \n",
       "1 -0.342475      0  \n",
       "2  1.160686      0  \n",
       "3  0.140534      0  \n",
       "4 -0.073403      0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2b8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Time'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ccdd3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "\n",
       "     Amount  Class  \n",
       "0  0.244964      0  \n",
       "1 -0.342475      0  \n",
       "2  1.160686      0  \n",
       "3  0.140534      0  \n",
       "4 -0.073403      0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29b2437a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9c26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1eacfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663, 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7708e0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    275190\n",
       "1       473\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60948d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "sns.countplot(data['Class'], color = 'green')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab79f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis = 1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22537221",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fa01baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Logistic Regression===========\n",
      "\n",
      " Accuracy: 0.9992200678359603\n",
      " Precision: 0.8870967741935484\n",
      " Recall: 0.6043956043956044\n",
      " F1 Score: 0.718954248366013\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55035     7]\n",
      " [   36    55]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.89      0.60      0.72        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.94      0.80      0.86     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Decision Tree Classifier===========\n",
      "\n",
      " Accuracy: 0.9989661364337148\n",
      " Precision: 0.6770833333333334\n",
      " Recall: 0.7142857142857143\n",
      " F1 Score: 0.6951871657754011\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55011    31]\n",
      " [   26    65]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.68      0.71      0.70        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.84      0.86      0.85     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Random Forest Classifier===========\n",
      "\n",
      " Accuracy: 0.9994377233235993\n",
      " Precision: 0.9054054054054054\n",
      " Recall: 0.7362637362637363\n",
      " F1 Score: 0.8121212121212122\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55035     7]\n",
      " [   24    67]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.91      0.74      0.81        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.95      0.87      0.91     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Support Vector Classifier===========\n",
      "\n",
      " Accuracy: 0.9993288955797798\n",
      " Precision: 0.9354838709677419\n",
      " Recall: 0.6373626373626373\n",
      " F1 Score: 0.7581699346405227\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55038     4]\n",
      " [   33    58]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.94      0.64      0.76        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.97      0.82      0.88     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========K-Nearest Neighbors Classifier===========\n",
      "\n",
      " Accuracy: 0.999419585366296\n",
      " Precision: 0.8831168831168831\n",
      " Recall: 0.7472527472527473\n",
      " F1 Score: 0.8095238095238096\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55033     9]\n",
      " [   23    68]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.88      0.75      0.81        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.94      0.87      0.90     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Gaussian Naive Bayes===========\n",
      "\n",
      " Accuracy: 0.9781618994068888\n",
      " Precision: 0.057279236276849645\n",
      " Recall: 0.7912087912087912\n",
      " F1 Score: 0.10682492581602375\n",
      "\n",
      " Confusion Matrix:\n",
      "[[53857  1185]\n",
      " [   19    72]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     55042\n",
      "           1       0.06      0.79      0.11        91\n",
      "\n",
      "    accuracy                           0.98     55133\n",
      "   macro avg       0.53      0.88      0.55     55133\n",
      "weighted avg       1.00      0.98      0.99     55133\n",
      "\n",
      "\n",
      "==========AdaBoost Classifier===========\n",
      "\n",
      " Accuracy: 0.9991112400921408\n",
      " Precision: 0.75\n",
      " Recall: 0.6923076923076923\n",
      " F1 Score: 0.7199999999999999\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55021    21]\n",
      " [   28    63]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.75      0.69      0.72        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.87      0.85      0.86     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Gradient Boosting Classifier===========\n",
      "\n",
      " Accuracy: 0.9986033772876499\n",
      " Precision: 0.71875\n",
      " Recall: 0.25274725274725274\n",
      " F1 Score: 0.37398373983739835\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55033     9]\n",
      " [   68    23]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.72      0.25      0.37        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.86      0.63      0.69     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Bagging Classifier===========\n",
      "\n",
      " Accuracy: 0.9995646890247221\n",
      " Precision: 0.958904109589041\n",
      " Recall: 0.7692307692307693\n",
      " F1 Score: 0.8536585365853658\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55039     3]\n",
      " [   21    70]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.96      0.77      0.85        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.98      0.88      0.93     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Extra Trees Classifier===========\n",
      "\n",
      " Accuracy: 0.9994739992382058\n",
      " Precision: 0.9305555555555556\n",
      " Recall: 0.7362637362637363\n",
      " F1 Score: 0.8220858895705522\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55037     5]\n",
      " [   24    67]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.93      0.74      0.82        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.97      0.87      0.91     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Stochastic Gradient Descent Classifier===========\n",
      "\n",
      " Accuracy: 0.9990568262202311\n",
      " Precision: 0.819672131147541\n",
      " Recall: 0.5494505494505495\n",
      " F1 Score: 0.6578947368421053\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55031    11]\n",
      " [   41    50]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.82      0.55      0.66        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.91      0.77      0.83     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n",
      "\n",
      "==========Voting Classifier===========\n",
      "\n",
      " Accuracy: 0.9994558612809026\n",
      " Precision: 0.9295774647887324\n",
      " Recall: 0.7252747252747253\n",
      " F1 Score: 0.8148148148148148\n",
      "\n",
      " Confusion Matrix:\n",
      "[[55037     5]\n",
      " [   25    66]]\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     55042\n",
      "           1       0.93      0.73      0.81        91\n",
      "\n",
      "    accuracy                           1.00     55133\n",
      "   macro avg       0.96      0.86      0.91     55133\n",
      "weighted avg       1.00      1.00      1.00     55133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"K-Nearest Neighbors Classifier\": KNeighborsClassifier(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"Bagging Classifier\": BaggingClassifier(),\n",
    "    \"Extra Trees Classifier\": ExtraTreesClassifier(),\n",
    "    \"Stochastic Gradient Descent Classifier\": SGDClassifier(),\n",
    "    \"Voting Classifier\": VotingClassifier(estimators=[\n",
    "        ('lr', LogisticRegression()),\n",
    "        ('dt', DecisionTreeClassifier()),\n",
    "        ('rf', RandomForestClassifier()),\n",
    "        ('svc', SVC()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ], voting='hard')\n",
    "}\n",
    "\n",
    "for name, clf in classifier.items():\n",
    "    print(f\"\\n=========={name}===========\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluation Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"\\n Accuracy: {accuracy}\")\n",
    "    print(f\" Precision: {precision}\")\n",
    "    print(f\" Recall: {recall}\")\n",
    "    print(f\" F1 Score: {f1}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\n Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b138944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnderSampling\n",
    "normal = data[data['Class'] == 0]\n",
    "fraud = data[data['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9628b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275190, 30)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155c053f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 30)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49ee9390",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sample = normal.sample(n=473)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06d0203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(473, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc354232",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([normal_sample, fraud], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afca792b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.872940</td>\n",
       "      <td>1.797359</td>\n",
       "      <td>0.696415</td>\n",
       "      <td>0.915173</td>\n",
       "      <td>-1.065011</td>\n",
       "      <td>-0.102628</td>\n",
       "      <td>-0.826427</td>\n",
       "      <td>1.250767</td>\n",
       "      <td>-0.613325</td>\n",
       "      <td>-0.568226</td>\n",
       "      <td>-0.604709</td>\n",
       "      <td>0.793920</td>\n",
       "      <td>0.901921</td>\n",
       "      <td>0.786029</td>\n",
       "      <td>1.498281</td>\n",
       "      <td>0.126029</td>\n",
       "      <td>0.344861</td>\n",
       "      <td>-0.129090</td>\n",
       "      <td>-0.048887</td>\n",
       "      <td>-0.422028</td>\n",
       "      <td>0.284888</td>\n",
       "      <td>0.357907</td>\n",
       "      <td>0.203695</td>\n",
       "      <td>0.074903</td>\n",
       "      <td>-0.650199</td>\n",
       "      <td>-0.533393</td>\n",
       "      <td>-0.649888</td>\n",
       "      <td>-0.077926</td>\n",
       "      <td>-0.338277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.428101</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>1.441699</td>\n",
       "      <td>-1.040944</td>\n",
       "      <td>-1.049091</td>\n",
       "      <td>-0.918465</td>\n",
       "      <td>0.887088</td>\n",
       "      <td>-0.307758</td>\n",
       "      <td>-1.754002</td>\n",
       "      <td>0.058590</td>\n",
       "      <td>-0.136647</td>\n",
       "      <td>0.246635</td>\n",
       "      <td>1.330315</td>\n",
       "      <td>-0.163795</td>\n",
       "      <td>0.505547</td>\n",
       "      <td>-1.474900</td>\n",
       "      <td>0.038805</td>\n",
       "      <td>0.675976</td>\n",
       "      <td>-0.666314</td>\n",
       "      <td>-0.039234</td>\n",
       "      <td>-0.390992</td>\n",
       "      <td>-0.914343</td>\n",
       "      <td>0.403096</td>\n",
       "      <td>0.712569</td>\n",
       "      <td>-0.350240</td>\n",
       "      <td>0.746873</td>\n",
       "      <td>-0.050828</td>\n",
       "      <td>0.066868</td>\n",
       "      <td>0.247883</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.509471</td>\n",
       "      <td>0.186499</td>\n",
       "      <td>-0.649117</td>\n",
       "      <td>-1.412331</td>\n",
       "      <td>3.344024</td>\n",
       "      <td>3.342335</td>\n",
       "      <td>0.161437</td>\n",
       "      <td>0.919614</td>\n",
       "      <td>-0.551644</td>\n",
       "      <td>-0.573091</td>\n",
       "      <td>-0.178191</td>\n",
       "      <td>-0.051617</td>\n",
       "      <td>-0.334646</td>\n",
       "      <td>0.472467</td>\n",
       "      <td>0.377562</td>\n",
       "      <td>-0.408156</td>\n",
       "      <td>-0.240503</td>\n",
       "      <td>-0.730033</td>\n",
       "      <td>0.445978</td>\n",
       "      <td>0.116182</td>\n",
       "      <td>-0.177459</td>\n",
       "      <td>-0.700154</td>\n",
       "      <td>0.050898</td>\n",
       "      <td>0.703631</td>\n",
       "      <td>-0.348343</td>\n",
       "      <td>0.274670</td>\n",
       "      <td>0.082447</td>\n",
       "      <td>0.142968</td>\n",
       "      <td>-0.320965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.374005</td>\n",
       "      <td>0.482576</td>\n",
       "      <td>1.770787</td>\n",
       "      <td>1.593996</td>\n",
       "      <td>-0.982856</td>\n",
       "      <td>0.970837</td>\n",
       "      <td>-0.897787</td>\n",
       "      <td>0.832278</td>\n",
       "      <td>2.581847</td>\n",
       "      <td>-1.146770</td>\n",
       "      <td>-1.794343</td>\n",
       "      <td>-3.097422</td>\n",
       "      <td>-0.285742</td>\n",
       "      <td>1.021929</td>\n",
       "      <td>-2.071840</td>\n",
       "      <td>-0.553979</td>\n",
       "      <td>1.309727</td>\n",
       "      <td>0.638226</td>\n",
       "      <td>1.177006</td>\n",
       "      <td>-0.277841</td>\n",
       "      <td>-0.257829</td>\n",
       "      <td>-0.238127</td>\n",
       "      <td>-0.030626</td>\n",
       "      <td>-0.545915</td>\n",
       "      <td>-0.244084</td>\n",
       "      <td>-0.317315</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>-0.020749</td>\n",
       "      <td>-0.243602</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.874560</td>\n",
       "      <td>-0.126414</td>\n",
       "      <td>-0.284692</td>\n",
       "      <td>1.565162</td>\n",
       "      <td>-0.253299</td>\n",
       "      <td>-0.041902</td>\n",
       "      <td>-0.200514</td>\n",
       "      <td>-0.001844</td>\n",
       "      <td>0.980059</td>\n",
       "      <td>-0.064889</td>\n",
       "      <td>-0.778900</td>\n",
       "      <td>1.384011</td>\n",
       "      <td>0.789562</td>\n",
       "      <td>-0.458333</td>\n",
       "      <td>-1.263095</td>\n",
       "      <td>-0.607654</td>\n",
       "      <td>0.054042</td>\n",
       "      <td>-1.177624</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>-0.176313</td>\n",
       "      <td>-0.477531</td>\n",
       "      <td>-1.100998</td>\n",
       "      <td>0.495456</td>\n",
       "      <td>1.030312</td>\n",
       "      <td>-0.334261</td>\n",
       "      <td>-1.163839</td>\n",
       "      <td>0.056655</td>\n",
       "      <td>-0.014529</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.872940  1.797359  0.696415  0.915173 -1.065011 -0.102628 -0.826427   \n",
       "1 -0.428101 -0.002632  1.441699 -1.040944 -1.049091 -0.918465  0.887088   \n",
       "2 -0.509471  0.186499 -0.649117 -1.412331  3.344024  3.342335  0.161437   \n",
       "3 -0.374005  0.482576  1.770787  1.593996 -0.982856  0.970837 -0.897787   \n",
       "4  1.874560 -0.126414 -0.284692  1.565162 -0.253299 -0.041902 -0.200514   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  1.250767 -0.613325 -0.568226 -0.604709  0.793920  0.901921  0.786029   \n",
       "1 -0.307758 -1.754002  0.058590 -0.136647  0.246635  1.330315 -0.163795   \n",
       "2  0.919614 -0.551644 -0.573091 -0.178191 -0.051617 -0.334646  0.472467   \n",
       "3  0.832278  2.581847 -1.146770 -1.794343 -3.097422 -0.285742  1.021929   \n",
       "4 -0.001844  0.980059 -0.064889 -0.778900  1.384011  0.789562 -0.458333   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.498281  0.126029  0.344861 -0.129090 -0.048887 -0.422028  0.284888   \n",
       "1  0.505547 -1.474900  0.038805  0.675976 -0.666314 -0.039234 -0.390992   \n",
       "2  0.377562 -0.408156 -0.240503 -0.730033  0.445978  0.116182 -0.177459   \n",
       "3 -2.071840 -0.553979  1.309727  0.638226  1.177006 -0.277841 -0.257829   \n",
       "4 -1.263095 -0.607654  0.054042 -1.177624  0.005128 -0.176313 -0.477531   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.357907  0.203695  0.074903 -0.650199 -0.533393 -0.649888 -0.077926   \n",
       "1 -0.914343  0.403096  0.712569 -0.350240  0.746873 -0.050828  0.066868   \n",
       "2 -0.700154  0.050898  0.703631 -0.348343  0.274670  0.082447  0.142968   \n",
       "3 -0.238127 -0.030626 -0.545915 -0.244084 -0.317315 -0.003429 -0.020749   \n",
       "4 -1.100998  0.495456  1.030312 -0.334261 -1.163839  0.056655 -0.014529   \n",
       "\n",
       "     Amount  Class  \n",
       "0 -0.338277      0  \n",
       "1  0.247883      0  \n",
       "2 -0.320965      0  \n",
       "3 -0.243602      0  \n",
       "4 -0.246361      0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "189d41cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    473\n",
       "1    473\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a8d7043",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_data.drop('Class', axis = 1)\n",
    "y = new_data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85dea3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f91808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Logistic Regression==========\n",
      "\n",
      " Accuaracy: 0.9526315789473684\n",
      "\n",
      " Precision: 0.979381443298969\n",
      "\n",
      " Recall: 0.9313725490196079\n",
      "\n",
      " F1 Score: 0.9547738693467337\n",
      "\n",
      "==========Decision Tree Classifier==========\n",
      "\n",
      " Accuaracy: 0.9315789473684211\n",
      "\n",
      " Precision: 0.9494949494949495\n",
      "\n",
      " Recall: 0.9215686274509803\n",
      "\n",
      " F1 Score: 0.9353233830845771\n"
     ]
    }
   ],
   "source": [
    "classifier = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "for name, clf in classifier.items():\n",
    "    print(f\"\\n=========={name}==========\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"\\n Accuaracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"\\n Precision: {precision_score(y_test, y_pred)}\")\n",
    "    print(f\"\\n Recall: {recall_score(y_test, y_pred)}\")\n",
    "    print(f\"\\n F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afcd75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OverSampling\n",
    "X = data.drop('Class', axis = 1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b90fe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663, 29)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "185a49f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d370fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_res, y_res = SMOTE().fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "37cbe2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    275190\n",
       "1    275190\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "004aa635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6ce7425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========Logistic Regression===========\n",
      "\n",
      " Accuaracy: 0.943230131908863\n",
      "\n",
      " Precision: 0.9730091586463832\n",
      "\n",
      " Recall: 0.9116775448611893\n",
      "\n",
      " F1 Score: 0.9413454228029172\n",
      "\n",
      "==========Decision Tree Classifier===========\n",
      "\n",
      " Accuaracy: 0.9980195501290018\n",
      "\n",
      " Precision: 0.9972949911951056\n",
      "\n",
      " Recall: 0.9987455229714742\n",
      "\n",
      " F1 Score: 0.9980197300292498\n"
     ]
    }
   ],
   "source": [
    "classifier = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "for name, clf in classifier.items():\n",
    "    print(f\"\\n=========={name}===========\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\n Accuaracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"\\n Precision: {precision_score(y_test, y_pred)}\")\n",
    "    print(f\"\\n Recall: {recall_score(y_test, y_pred)}\")\n",
    "    print(f\"\\n F1 Score: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bf88c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7503ae",
   "metadata": {},
   "source": [
    "### _Model Deployment_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a2a15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['credit_card_model.pkl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(dtc, \"credit_card_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d7e7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"credit_card_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea593502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AnacondaNavigator\\Lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([[-1.3598071336738,-0.0727811733098497,2.53634673796914,1.37815522427443,-0.338320769942518,0.462387777762292,0.239598554061257,0.0986979012610507,0.363786969611213,0.0907941719789316,-0.551599533260813,-0.617800855762348,-0.991389847235408,-0.311169353699879,1.46817697209427,-0.470400525259478,0.207971241929242,0.0257905801985591,0.403992960255733,0.251412098239705,-0.018306777944153,0.277837575558899,-0.110473910188767,0.0669280749146731,0.128539358273528,-0.189114843888824,0.133558376740387,-0.0210530534538215,149.62]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52b7f76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c54bf598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Transaction\n"
     ]
    }
   ],
   "source": [
    "if pred[0] == 0:\n",
    "    print(\"Normal Transaction\")\n",
    "else:\n",
    "    print(\"Fraud Transaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37efcc4",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc6e56",
   "metadata": {},
   "source": [
    "Credit card fraud is a significant concern for financial institutions and individuals alike. Fraudsters can steal card information and make unauthorized purchases, leading to financial losses.\n",
    "\n",
    "Problem: Fraudsters steal card information and make unauthorized purchases, causing financial losses.\n",
    "\n",
    "Solution: Build a system to detect fraudulent transactions.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Data Prep: Get transaction data, clean it (handle missing values), and create helpful features.\n",
    "Model Training: Choose and train a machine learning model (e.g., Decision Tree) to identify fraud based on transaction details.\n",
    "\n",
    "Evaluation: Test the model's accuracy in detecting fraud.\n",
    "\n",
    "Deployment: Use the model to analyze real-time transactions and flag potential fraud.\n",
    "\n",
    "Monitoring: Regularly update the model with new data to stay ahead of evolving fraud tactics.\n",
    "\n",
    "â€  Key Points:\n",
    "\n",
    "Choose the right model based on data and needs (interpretability vs. complexity).\n",
    "\n",
    "Balance the model to handle rare fraudulent transactions (class imbalance).\n",
    "\n",
    "Continuously improve the model with new data and techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ba1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
